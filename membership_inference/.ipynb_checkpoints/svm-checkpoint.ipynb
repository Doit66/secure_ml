{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "separate-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-honolulu",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "we'll use mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "narrative-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像データ数:(70000, 784)\n",
      "ラベルデータ数:(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.fetch_openml('mnist_784', version=1, data_home=\".\", return_X_y=True)\n",
    "imagedata, labeldata = mnist[0],mnist[1]\n",
    "print(\"画像データ数:\"+str(imagedata.shape))\n",
    "print(\"ラベルデータ数:\"+str(labeldata.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "searching-bracelet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist_data = mnist[0].values / 255\n",
    "mnist_label = mnist[1].values\n",
    "\n",
    "print(mnist_data.shape)\n",
    "print(mnist_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-semiconductor",
   "metadata": {},
   "source": [
    "We devide our data three: train data, shadow data, and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "mexican-veteran",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape is  (500, 784)\n",
      "shadow_data shape is  (500, 784)\n",
      "eval_data shape is  (500, 784)\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "idxs = list(range(mnist_data.shape[0]))\n",
    "random.shuffle(idxs)\n",
    "\n",
    "train_idx = idxs[:500]\n",
    "shadow_idx = idxs[500:1000]\n",
    "eval_idx = idxs[1000:1500]\n",
    "\n",
    "train_data = mnist_data[train_idx]\n",
    "shadow_data = mnist_data[shadow_idx]\n",
    "eval_data = mnist_data[eval_idx]\n",
    "\n",
    "train_label = mnist_label[train_idx]\n",
    "shadow_label = mnist_label[shadow_idx]\n",
    "eval_label = mnist_label[eval_idx]\n",
    "\n",
    "print(\"train_data shape is \", train_data.shape)\n",
    "print(\"shadow_data shape is \", shadow_data.shape)\n",
    "print(\"eval_data shape is \", eval_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-twelve",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Target model\n",
    "\n",
    "We assume that the target use SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fewer-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.992\n",
      "0.878\n"
     ]
    }
   ],
   "source": [
    "target_model = SVC(probability=True)\n",
    "target_model.fit(train_data, train_label)\n",
    "\n",
    "target_pred = target_model.predict(train_data)\n",
    "target_prob = target_model.predict_proba(train_data)\n",
    "\n",
    "eval_pred = target_model.predict(eval_data)\n",
    "eval_prob = target_model.predict_proba(eval_data)\n",
    "\n",
    "ac_score = metrics.accuracy_score(target_pred, train_label)\n",
    "print(ac_score)\n",
    "ac_score = metrics.accuracy_score(eval_pred, eval_label)\n",
    "print(ac_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-joshua",
   "metadata": {},
   "source": [
    "# Shadow model\n",
    "\n",
    "We also use SVM as shadow model and create 5 models with k-fold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "elegant-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,\n",
    "           random_state=42,\n",
    "           shuffle=True)\n",
    "\n",
    "in_probs = []\n",
    "out_probs = []\n",
    "shadow_in_labels = []\n",
    "shadow_out_labels = []\n",
    "\n",
    "for trn_idx, val_idx in kf.split(shadow_data):\n",
    "    in_data = shadow_data[trn_idx]\n",
    "    out_data =shadow_data[val_idx]\n",
    "    in_label = shadow_label[trn_idx]\n",
    "    out_label = shadow_label[val_idx]\n",
    "    \n",
    "    shadow_model = SVC(probability=True)\n",
    "    shadow_model.fit(in_data, in_label)\n",
    "    \n",
    "    in_prob = shadow_model.predict_proba(in_data)\n",
    "    out_prob = shadow_model.predict_proba(out_data)\n",
    "    \n",
    "    in_probs.append(in_prob)\n",
    "    out_probs.append(out_prob)\n",
    "    \n",
    "    shadow_in_labels.append(in_label)\n",
    "    shadow_out_labels.append(out_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-fireplace",
   "metadata": {},
   "source": [
    "create labels to train attack model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "psychological-seattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_probs = np.concatenate(in_probs)\n",
    "out_probs = np.concatenate(out_probs)\n",
    "\n",
    "in_labels = np.ones(in_probs.shape[0])\n",
    "out_labels = np.zeros(out_probs.shape[0])\n",
    "\n",
    "attack_data = np.concatenate([in_probs, out_probs])\n",
    "attack_label = np.concatenate([in_labels, out_labels])\n",
    "\n",
    "shadow_in_labels = np.concatenate(shadow_in_labels)\n",
    "shadow_out_labels = np.concatenate(shadow_out_labels)\n",
    "shadow_original_label = np.concatenate([shadow_in_labels,\n",
    "                                        shadow_out_labels])\n",
    "\n",
    "attack_data_idx = list(range(attack_data.shape[0]))\n",
    "random.shuffle(attack_data_idx)\n",
    "\n",
    "attack_data = attack_data[attack_data_idx]\n",
    "attack_label = attack_label[attack_data_idx]\n",
    "shadow_original_label = shadow_original_label[attack_data_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-minority",
   "metadata": {},
   "source": [
    "# Attack model\n",
    "\n",
    "We make SVM classifier for each label as attack model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "earlier-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(shadow_original_label)\n",
    "\n",
    "all_attack_true_label = np.zeros_like(shadow_original_label).astype(int)\n",
    "all_attack_preds = np.zeros_like(shadow_original_label).astype(int)\n",
    "\n",
    "attack_model_dict = {ul:None for ul in unique_labels}\n",
    "\n",
    "for label in unique_labels:\n",
    "\n",
    "    label_idx = np.where(shadow_original_label == label)[0]\n",
    "\n",
    "    attack_label_data = attack_data[label_idx]\n",
    "    attack_label_label = attack_label[label_idx]\n",
    "\n",
    "    attack_model = SVC(probability=True)\n",
    "    attack_model.fit(attack_label_data, attack_label_label)\n",
    "    attack_pred = attack_model.predict(attack_label_data)\n",
    "\n",
    "    all_attack_true_label[label_idx] = attack_label_label\n",
    "    all_attack_preds[label_idx] = attack_pred\n",
    "    \n",
    "    attack_model_dict[label] = attack_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aboriginal-associate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.89      0.37       132\n",
      "           1       0.99      0.84      0.91      2368\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.61      0.86      0.64      2500\n",
      "weighted avg       0.95      0.84      0.88      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(all_attack_preds, all_attack_true_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-cable",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "subjective-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_in = np.ones_like(train_label).astype(int)\n",
    "target_label_out = np.zeros_like(eval_label).astype(int)\n",
    "\n",
    "probs = np.concatenate([target_prob, eval_prob])\n",
    "label_in_out = np.concatenate([target_label_in, target_label_out])\n",
    "true_label = np.concatenate([train_label, eval_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "surrounded-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_out_label_pred = np.zeros_like(label_in_out).astype(int)\n",
    "\n",
    "for label, label_model in attack_model_dict.items():\n",
    "    label_idx = np.where(true_label == label)[0]\n",
    "    \n",
    "    predict_in_out_label = attack_model_dict[label].predict(probs[label_idx])\n",
    "    true_in_out_label = label_in_out[label_idx]\n",
    "    \n",
    "    in_out_label_pred[label_idx] = predict_in_out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "behind-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall f1 score is  0.706983441324694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.92      0.33       111\n",
      "           1       0.98      0.55      0.71       889\n",
      "\n",
      "    accuracy                           0.59      1000\n",
      "   macro avg       0.59      0.74      0.52      1000\n",
      "weighted avg       0.90      0.59      0.67      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"overall f1 score is \", metrics.f1_score(in_out_label_pred, label_in_out))\n",
    "print(metrics.classification_report(in_out_label_pred, label_in_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-trinity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
